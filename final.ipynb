{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KY27CotHmHF1",
    "outputId": "744de6b7-e4ef-4ca1-c00a-60eb454c5fb0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate peft bitsandbytes sentencepiece\n",
    "!pip install qwen-vl-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRmDdGsfmIHu",
    "outputId": "b7d0d04d-ef78-40d4-b3a5-67bdba44f9d6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate peft bitsandbytes qwen-vl-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UYI-MbeBmLNO",
    "outputId": "cf59fa84-872d-4f22-9f86-dd83c3793b86",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y bitsandbytes\n",
    "!pip install --upgrade pip\n",
    "!pip install unsloth transformers accelerate pillow datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3ccxDzCmOLV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JhsNVe_l2Kp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "70e14730-c432-4932-f4e3-c10a3cc383a6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Mc159JS_dwm"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"hugging_face_token\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "LORA BASED GRPO TRAINING"
   ],
   "metadata": {
    "id": "XfqQ7ADN-A3u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Patched GRPO for Qwen2-VL + LoRA (hybrid precision loader fixed)\n",
    "# - Auto device_map then force vision modules to GPU\n",
    "# - Larger LoRA coverage and rank for meaningful updates\n",
    "# - Debug mode (run_few_steps) to verify gradients\n",
    "#\n",
    "# Edit PATHS at the top (json_path, IMAGE_ROOT, save_path) before running.\n",
    "\n",
    "import os, json, time, gc, re, difflib\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from safetensors.torch import save_file as safe_save\n",
    "\n",
    "# -------------------------\n",
    "# PATHS (EDIT)\n",
    "# -------------------------\n",
    "json_path = \"/content/drive/MyDrive/plantfolder100/plant100.json\"\n",
    "IMAGE_ROOT = \"/content/drive/MyDrive/plantfolder100/Images/train\"\n",
    "existing_lora = None   # set path to continue from an existing adapter if desired\n",
    "save_path = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "base_model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# HYPERPARAMETERS (T4-friendly)\n",
    "# -------------------------\n",
    "train_batch_size = 1\n",
    "gradient_accumulation_steps = 8\n",
    "lr = 3e-5\n",
    "num_epochs = 3\n",
    "gen_max_tokens = 48\n",
    "kl_coeff = 0.02\n",
    "gamma_baseline = 0.99\n",
    "max_grad_norm = 1.0\n",
    "log_interval = 10\n",
    "max_seq_length = 512\n",
    "\n",
    "# LoRA config (bigger capacity)\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "use_4bit = True\n",
    "use_gradient_checkpointing = True\n",
    "cache_vision_embeddings = True\n",
    "\n",
    "use_amp = False\n",
    "scaler = None\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = \"cuda\" if use_gpu else \"cpu\"\n",
    "\n",
    "# DEBUG mode: run only a few steps to verify gradients (set False for full training)\n",
    "run_few_steps = False\n",
    "few_steps = 5\n",
    "\n",
    "# -------------------------\n",
    "# UTILITIES\n",
    "# -------------------------\n",
    "def clear_cache():\n",
    "    if use_gpu:\n",
    "        try: torch.cuda.empty_cache()\n",
    "        except: pass\n",
    "    gc.collect()\n",
    "\n",
    "def extract_tag(text, tag):\n",
    "    if not text: return \"\"\n",
    "    m = re.search(fr\"<{tag}>(.*?)</{tag}>\", text, re.S | re.I)\n",
    "    return m.group(1).strip() if m else \"\"\n",
    "\n",
    "def similarity(a,b):\n",
    "    a = (a or \"\").strip().lower(); b=(b or \"\").strip().lower()\n",
    "    if not a or not b: return 0.0\n",
    "    return difflib.SequenceMatcher(None,a,b).ratio()\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA + PROCESSOR\n",
    "# -------------------------\n",
    "with open(json_path, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "# reduce image size for memory\n",
    "try:\n",
    "    if hasattr(processor, \"image_processor\"):\n",
    "        processor.image_processor.size = {\"height\": 448, \"width\": 448}\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# DATASET (PlantDataset)\n",
    "# -------------------------\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self,data,processor,root,max_length=512):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.root = root\n",
    "        self.max_length = max_length\n",
    "        self.tokenized_cache = {}\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        img_name = os.path.basename(ex.get(\"image\",\"\"))\n",
    "        if idx in self.tokenized_cache:\n",
    "            cached = self.tokenized_cache[idx].copy(); cached[\"img_name\"]=img_name; return cached\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to open {img_path}: {e}\")\n",
    "        convs = ex.get(\"conversations\", [])\n",
    "        if len(convs) < 2:\n",
    "            user = ex.get(\"question\", \"<image>\\nQuestion: (no question found)\")\n",
    "            assistant = ex.get(\"answer\", \"<answer>(no answer)</answer>\")\n",
    "        else:\n",
    "            user = convs[0].get(\"value\",\"\")\n",
    "            assistant = convs[1].get(\"value\",\"\")\n",
    "        gold_perception = extract_tag(assistant, \"visual_perception\")\n",
    "        gold_answer = extract_tag(assistant, \"answer\")\n",
    "        if \"<image>\" not in user:\n",
    "            user = \"<image>\\n\" + user\n",
    "        messages = [\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":user},{\"type\":\"image\",\"image\":img}]},\n",
    "            {\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":assistant}]}\n",
    "        ]\n",
    "        encoded = self.processor.apply_chat_template(\n",
    "            messages, tokenize=True, return_tensors=\"pt\",\n",
    "            add_generation_prompt=False, return_dict=True, max_length=self.max_length, truncation=True\n",
    "        )\n",
    "        input_ids = encoded[\"input_ids\"]\n",
    "        labels = input_ids.clone(); labels[:] = -100\n",
    "        eos = self.processor.tokenizer.eos_token_id\n",
    "        pos = (input_ids==eos).nonzero(as_tuple=True)\n",
    "        if pos[0].numel()>0:\n",
    "            start = pos[1][0].item()+1\n",
    "            labels[0,start:] = input_ids[0,start:]\n",
    "        else:\n",
    "            labels[:] = -100\n",
    "        encoded[\"labels\"]=labels\n",
    "        encoded[\"user_text\"]=user; encoded[\"gold_answer\"]=gold_answer; encoded[\"gold_perception\"]=gold_perception; encoded[\"img_name\"]=img_name\n",
    "        result = {k:(v.squeeze(0) if torch.is_tensor(v) else v) for k,v in encoded.items()}\n",
    "        cache_entry = {k:(v.detach().cpu() if torch.is_tensor(v) else v) for k,v in result.items() if k!=\"img_name\"}\n",
    "        self.tokenized_cache[idx] = cache_entry\n",
    "        return result\n",
    "\n",
    "def collate_fn(batch):\n",
    "    out={}\n",
    "    keys=batch[0].keys()\n",
    "    for k in keys:\n",
    "        vals=[b[k] for b in batch]\n",
    "        if torch.is_tensor(vals[0]):\n",
    "            out[k]=torch.stack(vals)\n",
    "        else:\n",
    "            out[k]=vals\n",
    "    return out\n",
    "\n",
    "train_dataset = PlantDataset(raw_data, processor, IMAGE_ROOT, max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "# -------------------------\n",
    "# TARGET MODULES - broadened\n",
    "# -------------------------\n",
    "target_modules = [\n",
    "    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "    \"down_proj\",\"up_proj\",\"dense\",\"linear\",\n",
    "    \"proj_in\",\"proj_out\",\"wq\",\"wk\",\"wv\",\"wo\",\n",
    "    \"gated_act_proj\",\"mlp_dense_h_to_4h\",\"mlp_dense_4h_to_h\"\n",
    "]\n",
    "target_modules = list(dict.fromkeys(target_modules))\n",
    "print(\"Target modules:\", target_modules)\n",
    "\n",
    "# -------------------------\n",
    "# HYBRID MODEL LOADING: FIXED DEVICE MAP (auto -> force vision GPU)\n",
    "# -------------------------\n",
    "print(\"STEP 1: Loading model with automatic device_map to build a safe map...\")\n",
    "\n",
    "if use_4bit:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "# 1) Temporary load to get an auto device map\n",
    "tmp_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "auto_map = getattr(tmp_model, \"hf_device_map\", None) or getattr(tmp_model, \"device_map\", None) or {}\n",
    "del tmp_model\n",
    "clear_cache()\n",
    "\n",
    "# 2) Force certain vision modules to cuda if present in auto_map keys\n",
    "forced_gpu_keys = [\n",
    "    \"vision_tower\",\n",
    "    \"vision_tower.vision_model\",\n",
    "    \"vision_tower.vision_model.encoder\",\n",
    "    \"vision_tower.vision_model.embeddings\",\n",
    "    \"vision_proj\",\n",
    "    \"multi_modal_projector\",\n",
    "    \"multi_modal_projector.proj\",\n",
    "]\n",
    "\n",
    "for key in list(auto_map.keys()):\n",
    "    for fk in forced_gpu_keys:\n",
    "        if fk in key:\n",
    "            auto_map[key] = \"cuda\"\n",
    "\n",
    "print(\"Corrected device_map (sample):\")\n",
    "for k,v in list(auto_map.items())[:30]:\n",
    "    print(k, \"->\", v)\n",
    "print(\"Loading final model with corrected device_map...\")\n",
    "\n",
    "policy_base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=auto_map,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Prepare for kbit training if using 4-bit\n",
    "if use_4bit:\n",
    "    try:\n",
    "        policy_base = prepare_model_for_kbit_training(policy_base)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Ensure use_cache False and enable gradient checkpointing\n",
    "try:\n",
    "    policy_base.config.use_cache = False\n",
    "    if use_gradient_checkpointing:\n",
    "        policy_base.gradient_checkpointing_enable()\n",
    "        print(\"Enabled gradient checkpointing\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# -------------------------\n",
    "# Attach LoRA (higher capacity)\n",
    "# -------------------------\n",
    "print(\"Attaching higher-capacity LoRA...\")\n",
    "peft_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "policy_model = get_peft_model(policy_base, peft_cfg)\n",
    "\n",
    "# Optionally continue from existing adapter\n",
    "if existing_lora and os.path.isdir(existing_lora):\n",
    "    try:\n",
    "        policy_model = PeftModel.from_pretrained(policy_model, existing_lora, is_trainable=True)\n",
    "        print(\"Loaded existing LoRA adapter for continued training.\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not load existing LoRA (continuing from scratch):\", e)\n",
    "\n",
    "# Count trainable params\n",
    "trainable_params = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in policy_model.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} || All params: {all_params:,} || Trainable%: {100.0*trainable_params/all_params:.6f}%\")\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer\n",
    "# -------------------------\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    optimizer = bnb.optim.AdamW8bit([p for p in policy_model.parameters() if p.requires_grad], lr=lr, betas=(0.9,0.95))\n",
    "    print(\"Using 8-bit AdamW\")\n",
    "except Exception:\n",
    "    optimizer = torch.optim.AdamW([p for p in policy_model.parameters() if p.requires_grad], lr=lr)\n",
    "    print(\"Using standard AdamW\")\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# -------------------------\n",
    "# Reference model for KL\n",
    "# -------------------------\n",
    "ref_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=auto_map,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters(): p.requires_grad=False\n",
    "\n",
    "# -------------------------\n",
    "# Vision cache (optional)\n",
    "# -------------------------\n",
    "vision_cache = {}\n",
    "def precompute_vision_embeddings():\n",
    "    policy_model.eval()\n",
    "    unique_images = set(os.path.basename(ex.get(\"image\",\"\")) for ex in raw_data)\n",
    "    print(f\"Caching {len(unique_images)} images...\")\n",
    "    for i, img_name in enumerate(unique_images,1):\n",
    "        p = os.path.join(IMAGE_ROOT, img_name)\n",
    "        try:\n",
    "            im = Image.open(p).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        msg = [{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"<image>\"},{\"type\":\"image\",\"image\":im}]}]\n",
    "        enc = processor.apply_chat_template(msg, tokenize=True, return_tensors=\"pt\", add_generation_prompt=False, return_dict=True)\n",
    "        with torch.no_grad():\n",
    "            pv = enc.get(\"pixel_values\"); ig = enc.get(\"image_grid_thw\")\n",
    "            vision_cache[img_name] = {\"pixel_values\": pv.detach().cpu(), \"image_grid_thw\": ig.detach().cpu() if ig is not None else None}\n",
    "        if i % 100 == 0: clear_cache()\n",
    "    print(\"Cached images:\", len(vision_cache))\n",
    "    clear_cache()\n",
    "\n",
    "if cache_vision_embeddings:\n",
    "    precompute_vision_embeddings()\n",
    "\n",
    "# -------------------------\n",
    "# Fast generate helper\n",
    "# -------------------------\n",
    "@torch.inference_mode()\n",
    "def fast_generate(model, input_ids, attention_mask, pixel_values, image_grid_thw, max_new_tokens):\n",
    "    return model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pixel_values=pixel_values,\n",
    "        image_grid_thw=image_grid_thw,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Reward helpers\n",
    "# -------------------------\n",
    "def rouge_l_simple(a, b):\n",
    "    if not a or not b: return 0.0\n",
    "    a,b=a.strip().lower(),b.strip().lower()\n",
    "    la,lb=len(a),len(b)\n",
    "    dp=[[0]*(lb+1) for _ in range(la+1)]\n",
    "    for i in range(1,la+1):\n",
    "        for j in range(1,lb+1):\n",
    "            if a[i-1]==b[j-1]:\n",
    "                dp[i][j]=dp[i-1][j-1]+1\n",
    "            else:\n",
    "                dp[i][j]=max(dp[i-1][j], dp[i][j-1])\n",
    "    lcs = dp[la][lb]\n",
    "    return (2.0*lcs)/(la+lb+1e-12)\n",
    "\n",
    "def jaccard(a,b):\n",
    "    a_set=set((a or \"\").lower().split()); b_set=set((b or \"\").lower().split())\n",
    "    if not a_set and not b_set: return 0.0\n",
    "    return len(a_set & b_set)/max(1,len(a_set|b_set))\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN LOOP (GRPO)\n",
    "# -------------------------\n",
    "running_baseline = 0.0\n",
    "baseline_init = False\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"\\nSTARTING IMPROVED HYBRID GRPO (FIXED LOADER)\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        global_step += 1\n",
    "        tensors = {k:v for k,v in batch.items() if torch.is_tensor(v)}\n",
    "        gold_answer = batch[\"gold_answer\"][0]\n",
    "        gold_perception = batch[\"gold_perception\"][0]\n",
    "        img_name = batch[\"img_name\"][0]\n",
    "        user_text = batch.get(\"user_text\",[None])[0]\n",
    "\n",
    "        # vision inputs (from cache or on-the-fly)\n",
    "        if img_name in vision_cache:\n",
    "            vis = vision_cache[img_name]\n",
    "            pixel_values = vis[\"pixel_values\"].to(device)\n",
    "            image_grid_thw = vis.get(\"image_grid_thw\")\n",
    "            if image_grid_thw is not None: image_grid_thw = image_grid_thw.to(device)\n",
    "        else:\n",
    "            pixel_values = tensors[\"pixel_values\"].to(device)\n",
    "            image_grid_thw = tensors.get(\"image_grid_thw\", None)\n",
    "            if image_grid_thw is not None: image_grid_thw = image_grid_thw.to(device)\n",
    "\n",
    "        input_ids = tensors[\"input_ids\"].to(device)\n",
    "        attn = tensors[\"attention_mask\"].to(device)\n",
    "\n",
    "        # ========== ROLLOUT PHASE ==========\n",
    "        policy_model.eval()\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type='cuda' if use_gpu else 'cpu', enabled=use_amp):\n",
    "            gen_ids = fast_generate(policy_model, input_ids, attn, pixel_values, image_grid_thw, gen_max_tokens)\n",
    "\n",
    "        gen_text = processor.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "        pred_perception = extract_tag(gen_text, \"visual_perception\")\n",
    "        pred_answer1 = extract_tag(gen_text, \"answer\")\n",
    "\n",
    "        # self-rollout (no image) using only perception\n",
    "        if user_text:\n",
    "            qm = re.search(r\"Question\\s*:\\s*(.*)\", user_text, re.S | re.I)\n",
    "            question_text = qm.group(1).strip() if qm else user_text\n",
    "        else:\n",
    "            prompt_text = processor.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            qm = re.search(r\"Question\\s*:\\s*(.*)\", prompt_text, re.S | re.I)\n",
    "            question_text = qm.group(1).strip() if qm else prompt_text\n",
    "\n",
    "        perception_prompt = (\n",
    "            \"You previously saw the image and described it as:\\n\"\n",
    "            f\"<visual_perception>{pred_perception}</visual_perception>\\n\\n\"\n",
    "            \"Using ONLY the above perception, answer the question accurately. Do NOT invent new perception.\\n\"\n",
    "            f\"Question: {question_text}\\n\"\n",
    "            \"Give final answer inside <answer> tags.\"\n",
    "        )\n",
    "\n",
    "        msg2 = [{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":perception_prompt}]}]\n",
    "        enc2 = processor.apply_chat_template(msg2, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True, return_dict=True, max_length=max_seq_length, truncation=True)\n",
    "        input_ids2 = enc2[\"input_ids\"].to(device)\n",
    "        attn2 = enc2[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type='cuda' if use_gpu else 'cpu', enabled=use_amp):\n",
    "            gen2_ids = policy_model.generate(input_ids=input_ids2, attention_mask=attn2, max_new_tokens=gen_max_tokens, do_sample=False, num_beams=1, pad_token_id=processor.tokenizer.pad_token_id)\n",
    "\n",
    "        gen2_text = processor.tokenizer.batch_decode(gen2_ids, skip_special_tokens=True)[0]\n",
    "        pred_answer2 = extract_tag(gen2_text, \"answer\")\n",
    "\n",
    "        # ------------------ COMPUTE RICH REWARD ------------------\n",
    "        ans1_score = 0.6 * rouge_l_simple(pred_answer1, gold_answer) + 0.4 * jaccard(pred_answer1, gold_answer)\n",
    "        ans2_score = 0.75 * rouge_l_simple(pred_answer2, gold_answer) + 0.25 * jaccard(pred_answer2, gold_answer)\n",
    "        perc_score = 0.7 * rouge_l_simple(pred_perception, gold_perception) + 0.3 * jaccard(pred_perception, gold_perception)\n",
    "        len_penalty = -0.02 * max(0, len(pred_answer1.split()) - len(gold_answer.split()) - 10)\n",
    "\n",
    "        reward = (0.25*ans1_score + 0.55*ans2_score + 0.18*perc_score + 0.02*len_penalty)\n",
    "        reward = float(max(0.0, min(1.0, reward)))\n",
    "\n",
    "        # baseline & advantage\n",
    "        if not baseline_init:\n",
    "            running_baseline = reward; baseline_init=True\n",
    "        else:\n",
    "            running_baseline = gamma_baseline * running_baseline + (1.0 - gamma_baseline) * reward\n",
    "        advantage = float(max(min(reward - running_baseline, 10.0), -10.0))\n",
    "\n",
    "        # ========== COMPUTE POLICY LOSS ==========\n",
    "        prompt_len = input_ids.shape[1]\n",
    "        cont_len = gen_ids.shape[1] - prompt_len\n",
    "        if cont_len <= 0:\n",
    "            print(f\"[Warning] cont_len <= 0; skipping step.\")\n",
    "            continue\n",
    "\n",
    "        labels_pol = gen_ids.clone().to(device)\n",
    "        labels_pol[0, :prompt_len] = -100\n",
    "\n",
    "        pad_id = processor.tokenizer.pad_token_id\n",
    "        attn_full = (gen_ids != pad_id).long().to(device)\n",
    "\n",
    "        policy_model.train()\n",
    "        # IMPORTANT: enable autocast as appropriate\n",
    "        with torch.amp.autocast(device_type='cuda' if use_gpu else 'cpu', enabled=use_amp):\n",
    "            out_pol = policy_model(input_ids=gen_ids.to(device), attention_mask=attn_full, pixel_values=pixel_values if pixel_values is not None else None, image_grid_thw=image_grid_thw if image_grid_thw is not None else None, labels=labels_pol)\n",
    "            avg_neglog_pol = out_pol.loss\n",
    "\n",
    "            with torch.no_grad():\n",
    "                labels_ref = gen_ids.clone().to(device); labels_ref[0, :prompt_len] = -100\n",
    "                out_ref = ref_model(input_ids=gen_ids.to(device), attention_mask=attn_full, pixel_values=pixel_values if pixel_values is not None else None, image_grid_thw=image_grid_thw if image_grid_thw is not None else None, labels=labels_ref)\n",
    "                avg_neglog_ref = out_ref.loss\n",
    "\n",
    "            sum_logprob_pol = -avg_neglog_pol * cont_len\n",
    "            policy_loss = -(advantage * sum_logprob_pol)\n",
    "            kl_pen = kl_coeff * (avg_neglog_pol - avg_neglog_ref)\n",
    "            total_loss = (policy_loss + kl_pen) / gradient_accumulation_steps\n",
    "\n",
    "        # backward\n",
    "        total_loss.backward()\n",
    "\n",
    "        # gradient accumulation step\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_([p for p in policy_model.parameters() if p.requires_grad], max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # logging\n",
    "        if global_step % log_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            try:\n",
    "                gpu_mem = torch.cuda.memory_allocated()/1e9 if use_gpu else 0\n",
    "            except Exception:\n",
    "                gpu_mem = 0\n",
    "            print(f\"[Step {global_step}] reward={reward:.3f} ans1={ans1_score:.3f} self={ans2_score:.3f} perc={perc_score:.3f} adv={advantage:.3f} loss={(total_loss.item()*gradient_accumulation_steps):.4f} | GPU {gpu_mem:.2f}GB\")\n",
    "\n",
    "        if global_step % 50 == 0: clear_cache()\n",
    "\n",
    "        # debug quick exit\n",
    "        if run_few_steps and global_step >= few_steps:\n",
    "            print(\"Completed debug few steps, exiting training loop.\")\n",
    "            break\n",
    "\n",
    "    if run_few_steps and global_step >= few_steps:\n",
    "        break\n",
    "\n",
    "    # epoch-end leftover grads\n",
    "    any_grads = any((p.grad is not None) for p in policy_model.parameters() if p.requires_grad)\n",
    "    if any_grads:\n",
    "        torch.nn.utils.clip_grad_norm_([p for p in policy_model.parameters() if p.requires_grad], max_grad_norm)\n",
    "        optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "# -------------------------\n",
    "# SAVE LoRA (safetensors) + processor\n",
    "# -------------------------\n",
    "print(\"Saving LoRA adapter and processor...\")\n",
    "adapter_state = {k:v.cpu() for k,v in policy_model.state_dict().items() if \"lora\" in k or \"alpha\" in k}\n",
    "adapter_config = {\"r\": LORA_R, \"lora_alpha\": LORA_ALPHA, \"target_modules\": target_modules, \"lora_dropout\": LORA_DROPOUT}\n",
    "safe_save(adapter_state, os.path.join(save_path, \"adapter_model.safetensors\"))\n",
    "with open(os.path.join(save_path, \"adapter_config.json\"), \"w\") as f:\n",
    "    json.dump(adapter_config, f, indent=2)\n",
    "processor.save_pretrained(save_path)\n",
    "print(\"Saved to:\", save_path)\n",
    "\n",
    "print(\"Training finished.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949,
     "referenced_widgets": [
      "fe0884da35694ac59c42b8550e79dbc1",
      "7c4136871d014bbea9eaa0820cb1f032",
      "99840ab0324a4002822886d0dd26f6be",
      "db807a4f39974f3db2324291ab283c03",
      "a663474c9be74a6d9d3dfd32a2be294f",
      "a69e137511af4d97bb38653f25ccbc34",
      "4a845dcdca444dfea3d7b22347589b58",
      "88d9e585716545d29ef9faaf19f882a3",
      "5caa86010ff54e5f9ca870738d3c891a",
      "7f34def11f1b43bbb2ca9d10c13b6cda",
      "dccc3a59448546eb986bdff3c8cf4623",
      "a5d9d700d83341aa90c4fc891f26a5c8",
      "cf70ccaeee90422997bb2634b27ac67f",
      "1588e8b19de841f8bc2fab0830d99f49",
      "6a2670da6fda4f85baacdf1b0cea7715",
      "a0d09b2becda4163a1ac57250038743d",
      "db55cb1801d547b1b5a60cc47ad6e361",
      "72b0ca2b77964890a411cb1639f2084d",
      "3dae5df1017c446093255de21f265596",
      "fb9268d06367483980ecfe583453185c",
      "5691396fc9b4413f948b63cb9e0efd95",
      "655e9dfe30824336ba3d4117ea577270",
      "5feabba6b8f049c5adc81b6fd3b70432",
      "fa5fe2a39db74ff1905e2f3fb9890c22",
      "8b43e4361db2494a9eeaa106e1f28759",
      "18ec6e0f7274434094f7899b4eab68b3",
      "5f583d2401794566857f512df7e627ab",
      "cf0debeee1134592a202d7142ceb4eff",
      "aef749ebee884b8e814581fc8d233637",
      "62b17d992ff044e388731229850a433a",
      "9e025eea0f2448699b91896995364939",
      "ba3240ac2b9140a48e0add5f2212f41f",
      "c66be0b5c9c442879e9ff5a03a7ad51d"
     ]
    },
    "id": "fgALK86L1AXu",
    "outputId": "8166d8d5-8518-404e-eb0b-9d870aa84900"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "base_model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "policy_base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0032b72e8fb947eba87b629d458bc15f",
      "f011967317a24ac2829ecfeb67c543f1",
      "9e1c42d3f0cd498db2969fd07175a4c7",
      "c175c3ea691446159956f88f07507c69",
      "54fd422c5a8b4caab84f3325098d3342",
      "91479535b937471c8eeaae66fbc48758",
      "526199275d2f493cb8980704e9fadc81",
      "7d092d9d517f41709035d1a99e8ec233",
      "d39c8efc642b4f4492103688e9049e82",
      "56302c4b09ef444db5d0e4a829c64662",
      "099466bd961541fe94d4e918b3f60c7e"
     ]
    },
    "id": "PTbkRBIyo3W8",
    "outputId": "b753579c-d512-40fa-e5b1-10a0a73c9efc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "count = 0\n",
    "for name, module in policy_base.named_modules():\n",
    "    print(name)\n",
    "    count += 1\n",
    "    if count >= 300:\n",
    "        break\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdjskxTPpUUu",
    "outputId": "094142e3-a8a0-4f1d-a5fe-426341cb1716",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import safetensors.torch as st\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------\n",
    "base_model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "lora_path = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "lora_weights = f\"{lora_path}/adapter_model.safetensors\"   # Auto-saved by your script\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD PROCESSOR\n",
    "# ---------------------------------------------------------\n",
    "processor = AutoProcessor.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4-BIT QUANTIZATION CONFIG (T4 Friendly)\n",
    "# ---------------------------------------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# BASE MODEL LOADER\n",
    "# ---------------------------------------------------------\n",
    "def load_base():\n",
    "    print(\"\\nLoading BASE model...\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"Base model loaded.\\n\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MANUAL LORA LOADER (FIX FOR MISSING peft_type IN CONFIG)\n",
    "# ---------------------------------------------------------\n",
    "def load_lora():\n",
    "    print(\"\\nLoading LoRA model (manual load)...\")\n",
    "\n",
    "    # 1) Load base model again\n",
    "    base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # 2) Recreate EXACT LoRA config used in your training\n",
    "    lora_cfg = LoraConfig(\n",
    "        peft_type=\"LORA\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "            \"down_proj\",\"up_proj\",\"dense\",\"linear\",\n",
    "            \"proj_in\",\"proj_out\",\"wq\",\"wk\",\"wv\",\"wo\",\n",
    "            \"gated_act_proj\",\"mlp_dense_h_to_4h\",\"mlp_dense_4h_to_h\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "    # 3) Load LoRA weights manually\n",
    "    print(\"Loading LoRA weights from:\", lora_weights)\n",
    "    weights = st.load_file(lora_weights, device=\"cpu\")\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(weights, strict=False)\n",
    "    print(\"Loaded LoRA weights.\")\n",
    "    print(\"Missing keys:\", len(missing))\n",
    "    print(\"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PROMPT TEMPLATE FOR REASONING\n",
    "# ---------------------------------------------------------\n",
    "def build_prompt(question):\n",
    "    return (\n",
    "        \"Give a short reasoning summary (not full chain-of-thought).\\n\"\n",
    "        \"Then give the final answer inside <answer> tags.\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ASK FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def ask(model, img, question):\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    enc = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    # Ensure tensors have batch dimension\n",
    "    def ensure_batch(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, list):\n",
    "            x = x[0]\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return x.to(model.device)\n",
    "\n",
    "    input_ids = ensure_batch(enc[\"input_ids\"])\n",
    "    attention_mask = ensure_batch(enc[\"attention_mask\"])\n",
    "    pixel_values = ensure_batch(enc.get(\"pixel_values\"))\n",
    "    image_grid_thw = ensure_batch(enc.get(\"image_grid_thw\"))\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pixel_values=pixel_values,\n",
    "        image_grid_thw=image_grid_thw,\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "\n",
    "    return processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UPLOAD IMAGE\n",
    "# ---------------------------------------------------------\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "print(\"\\nImage loaded:\", image_path)\n",
    "\n",
    "\n",
    "question = \"What pest is present in the image? Explain reasoning and prevention.\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RUN BASE MODEL\n",
    "# ---------------------------------------------------------\n",
    "base_model = load_base()\n",
    "base_output = ask(base_model, img, question)\n",
    "\n",
    "print(\"\\n================ BASE MODEL OUTPUT ================\\n\")\n",
    "print(base_output)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RUN LORA MODEL\n",
    "# ---------------------------------------------------------\n",
    "lora_model = load_lora()\n",
    "lora_output = ask(lora_model, img, question)\n",
    "\n",
    "print(\"\\n================ LORA MODEL OUTPUT ================\\n\")\n",
    "print(lora_output)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "eb40ed3284fe454588c9922023c48d99",
      "12f5420b41544cf2850714211173dc73",
      "da77052e68074fa885ab3d1bad03d683",
      "b2933d38501c47f3b4396fdef8a6cf45",
      "9b0cbfd38d194b0c83e9237fe6f329ce",
      "6e761dd62c5c473aaee706078e444178",
      "349329ef867d4be0a17a3154b0fc02fc",
      "1e6d184c0b8645059c132e9d29a0e4b2",
      "c08da55d533f4235bbda2fff3a954a17",
      "45e64cc9d1104b8d9137290957c8b77e",
      "304335dd3a7b4641a8c1d5caf14b971f",
      "59413124d0204a2eb5f06052c8a38681",
      "17e1195666f9404bb4ec2e3e1de52a5d",
      "ee7259e8b0cd4149962ca66c72b71edb",
      "406df14af1cc43b5b0ec78ffdc10b119",
      "2b99c4acca1d46ac8fb9d55e9282a38e",
      "b3e7e32f5882456fb3b8f2959eae55dd",
      "34cb7f9e24ec45649e92a577d0afe61d",
      "dc19eb0d337a415eb8a5b3152b77c3dd",
      "790d6ef6bab847bfb377e4d75dfe57e9",
      "a697a154881a48499acb10df231975e2",
      "286e3aae78d245b8945ae9ae665586a4"
     ]
    },
    "id": "2AvsfhhB_TTO",
    "outputId": "3abe259d-5aa1-436a-8491-17a34508601a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import safetensors.torch as st\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------\n",
    "base_model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "lora_path = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "lora_weights = f\"{lora_path}/adapter_model.safetensors\"   # Auto-saved by your script\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD PROCESSOR\n",
    "# ---------------------------------------------------------\n",
    "processor = AutoProcessor.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4-BIT QUANTIZATION CONFIG (T4 Friendly)\n",
    "# ---------------------------------------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# BASE MODEL LOADER\n",
    "# ---------------------------------------------------------\n",
    "def load_base():\n",
    "    print(\"\\nLoading BASE model...\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"Base model loaded.\\n\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# MANUAL LORA LOADER (FIX FOR MISSING peft_type IN CONFIG)\n",
    "# ---------------------------------------------------------\n",
    "def load_lora():\n",
    "    print(\"\\nLoading LoRA model (manual load)...\")\n",
    "\n",
    "    # 1) Load base model again\n",
    "    base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # 2) Recreate EXACT LoRA config used in your training\n",
    "    lora_cfg = LoraConfig(\n",
    "        peft_type=\"LORA\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "            \"down_proj\",\"up_proj\",\"dense\",\"linear\",\n",
    "            \"proj_in\",\"proj_out\",\"wq\",\"wk\",\"wv\",\"wo\",\n",
    "            \"gated_act_proj\",\"mlp_dense_h_to_4h\",\"mlp_dense_4h_to_h\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "    # 3) Load LoRA weights manually\n",
    "    print(\"Loading LoRA weights from:\", lora_weights)\n",
    "    weights = st.load_file(lora_weights, device=\"cpu\")\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(weights, strict=False)\n",
    "    print(\"Loaded LoRA weights.\")\n",
    "    print(\"Missing keys:\", len(missing))\n",
    "    print(\"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PROMPT TEMPLATE FOR REASONING\n",
    "# ---------------------------------------------------------\n",
    "def build_prompt(question):\n",
    "    return (\n",
    "        \"Give a short reasoning summary (not full chain-of-thought).\\n\"\n",
    "        \"Then give the final answer inside <answer> tags.\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ASK FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def ask(model, img, question):\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    enc = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    # Ensure tensors have batch dimension\n",
    "    def ensure_batch(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, list):\n",
    "            x = x[0]\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return x.to(model.device)\n",
    "\n",
    "    input_ids = ensure_batch(enc[\"input_ids\"])\n",
    "    attention_mask = ensure_batch(enc[\"attention_mask\"])\n",
    "    pixel_values = ensure_batch(enc.get(\"pixel_values\"))\n",
    "    image_grid_thw = ensure_batch(enc.get(\"image_grid_thw\"))\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pixel_values=pixel_values,\n",
    "        image_grid_thw=image_grid_thw,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "    return processor.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UPLOAD IMAGE\n",
    "# ---------------------------------------------------------\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "print(\"\\nImage loaded:\", image_path)\n",
    "\n",
    "\n",
    "question = \"What pest is present in the image? Explain reasoning and prevention.\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RUN BASE MODEL\n",
    "# ---------------------------------------------------------\n",
    "base_model = load_base()\n",
    "base_output = ask(base_model, img, question)\n",
    "\n",
    "print(\"\\n================ BASE MODEL OUTPUT ================\\n\")\n",
    "print(base_output)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RUN LORA MODEL\n",
    "# ---------------------------------------------------------\n",
    "lora_model = load_lora()\n",
    "lora_output = ask(lora_model, img, question)\n",
    "\n",
    "print(\"\\n================ LORA MODEL OUTPUT ================\\n\")\n",
    "print(lora_output)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954,
     "referenced_widgets": [
      "8d3fc0e88b39454cac665f707ef4ad94",
      "0247b5279ab14501a7186fbfc1a46bb5",
      "43022593244f4807855f6e9d05fe1106",
      "d098d85ac65c4a9a91a73a350c3d2510",
      "b7ed53df0e9f47b18e0e4322dd6c56bb",
      "df0fd729a662491e8a4abcb423a0584c",
      "b2238b780c2f46d6bb7074b8a8b44040",
      "b44a82de1d604af68181a2d3277d9708",
      "d1c7199b423242d49c8dc9fd69f43a43",
      "d3259bf63c954e24b17fdfec14566a4f",
      "4f8fcb1ac36d4aacaa1f29f45ffad6cf",
      "a38892b578d3410fa6432ce19f3e7e50",
      "0a641520b45b45eb9cd8a977cb45a3e4",
      "240c760066d24d19a57aba864abbe990",
      "c2ad1b7250de4602965a91412e446c19",
      "86da4aac5ecf4d49b7ea63fdb1296a8e",
      "1486c38b5b0c4871948297e3d05ce758",
      "eb45ba2f4f0f4a479e5956d34bb4788c",
      "efaffd3766444fb7bc81d1401e138374",
      "6d33502efb8d4718ac03fa07cd0a4d49",
      "b8dfc28078e847028d7a2d1583217a55",
      "d5266beda8014b2d904c838774eeea28"
     ]
    },
    "id": "3MD4iYyvkotd",
    "outputId": "37c862ba-1755-4859-d017-8cd5d856bc64",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "COMPARISON BETWEEN BASE MODEL AND FINE TUNED MODEL"
   ],
   "metadata": {
    "id": "cehlDYQL9ity"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import safetensors.torch as st\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "\n",
    "# =========================================================\n",
    "# PATHS\n",
    "# =========================================================\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "LORA_PATH = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "LORA_WEIGHTS = f\"{LORA_PATH}/adapter_model.safetensors\"\n",
    "\n",
    "# =========================================================\n",
    "# LOAD PROCESSOR\n",
    "# =========================================================\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 4-BIT CONFIG (T4 SAFE)\n",
    "# =========================================================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# LOAD BASE MODEL\n",
    "# =========================================================\n",
    "def load_base():\n",
    "    print(\"\\nLoading BASE model...\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# LOAD BASE + LoRA MODEL\n",
    "# =========================================================\n",
    "def load_lora():\n",
    "    print(\"\\nLoading BASE + LoRA model...\")\n",
    "\n",
    "    base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        peft_type=\"LORA\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "            \"down_proj\",\"up_proj\",\"dense\",\"linear\",\n",
    "            \"proj_in\",\"proj_out\",\"wq\",\"wk\",\"wv\",\"wo\",\n",
    "            \"gated_act_proj\",\"mlp_dense_h_to_4h\",\"mlp_dense_4h_to_h\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "    weights = st.load_file(LORA_WEIGHTS, device=\"cpu\")\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# CLEAN OUTPUT (ABSOLUTE GUARANTEE)\n",
    "# =========================================================\n",
    "def clean_text(text):\n",
    "    text = text.split(\"assistant\")[-1]\n",
    "\n",
    "    # Remove markdown headings / tags / bullets\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"^#+.*$\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"[-\u2022*]\\s*\", \"\", text)\n",
    "\n",
    "    # Remove repeated labels\n",
    "    text = re.sub(\n",
    "        r\"(?i)(image description|pest description|pest identification|prevention methods|analysis).*?:\",\n",
    "        \"\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # Normalize spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# ASK FUNCTION (FINAL, STABLE)\n",
    "# =========================================================\n",
    "def ask(model, img, question):\n",
    "\n",
    "    prompt = (\n",
    "        \"Answer in ONE short paragraph.\\n\"\n",
    "        \"Use plain text only.\\n\"\n",
    "        \"Do not use headings, bullet points, markdown, or labels.\\n\"\n",
    "        \"Stop after completing the answer.\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items() if v is not None}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            repetition_penalty=1.15,   # PREVENT HEADER LOOPS\n",
    "            length_penalty=1.1,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    decoded = processor.tokenizer.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return clean_text(decoded)\n",
    "\n",
    "# =========================================================\n",
    "# UPLOAD IMAGE\n",
    "# =========================================================\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# =========================================================\n",
    "# QUESTION\n",
    "# =========================================================\n",
    "question = \"What pest is present in the image and how can it be prevented?\"\n",
    "\n",
    "# =========================================================\n",
    "# RUN BASE MODEL\n",
    "# =========================================================\n",
    "base_model = load_base()\n",
    "print(\"\\n\ud83d\udfe6 BASE MODEL OUTPUT\")\n",
    "print(ask(base_model, img, question))\n",
    "\n",
    "# =========================================================\n",
    "# RUN LoRA MODEL\n",
    "# =========================================================\n",
    "lora_model = load_lora()\n",
    "print(\"\\n\ud83d\udfe9 LORA MODEL OUTPUT\")\n",
    "print(ask(lora_model, img, question))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329,
     "referenced_widgets": [
      "2622a5e49aa64ac9afc8591f4cf0f1dd",
      "35e7dadd281442f5a2afdeefa0d0fd07",
      "009aca38962f4ec3bd68d4759158dc2f",
      "54646d3eaaba4559b615e25cf47403ab",
      "d27bf2b46ded4220bfa64a4d2c5a0b5c",
      "067a78ced8ff4c968acd3fcc55e895db",
      "7ee95d4ff8a94a62a24f4fc184d45d0c",
      "1fb0d42532b34fd2890c87f40652b2a4",
      "20163317583249a2bdf5a16af6db1f52",
      "47840eaab32748d38a2ababe56176d25",
      "7885d60d6fe24330b226b9ba53fe1ee6",
      "9ae73e2a137c4bad83667808e5b6ae96",
      "9dfdbcff48d44d92adca6d581516c880",
      "b9e44736302243d9be0487c38869bddf",
      "875293ed0c714455b18b79b32f508dcf",
      "9f961152d4f34b51a82258d4272b4045",
      "b7d93b8d1671419bb2e9b3caa5d107b7",
      "3a5b35c9d7704ecf9607828277b0b8ea",
      "ad088fd444634b67afeed7ebe59d5b2b",
      "c2ad66f0494a4f46b6fda8f86a823d21",
      "3ed16fbf3ea24b6ab5cf63eac1d67fe5",
      "ac90bde010934dfda327e9ac567f6a1e"
     ]
    },
    "id": "RPBpTWURAuy0",
    "outputId": "a8661d51-9f51-4519-e469-2e17056ace6a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "INFERENCE CODE"
   ],
   "metadata": {
    "id": "1JmiKz9q9cGI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import safetensors.torch as st\n",
    "from PIL import Image\n",
    "from google.colab import files\n",
    "\n",
    "# =========================================================\n",
    "# PATHS\n",
    "# =========================================================\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "LORA_PATH = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "LORA_WEIGHTS = f\"{LORA_PATH}/adapter_model.safetensors\"\n",
    "\n",
    "# =========================================================\n",
    "# LOAD PROCESSOR\n",
    "# =========================================================\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 4-BIT CONFIG (T4 SAFE)\n",
    "# =========================================================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# LOAD BASE + LoRA MODEL (ONLY MODEL USED)\n",
    "# =========================================================\n",
    "def load_lora_model():\n",
    "    print(\"\\nLoading YOUR LoRA-trained model...\")\n",
    "\n",
    "    base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        peft_type=\"LORA\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "            \"down_proj\",\"up_proj\",\"dense\",\"linear\",\n",
    "            \"proj_in\",\"proj_out\",\"wq\",\"wk\",\"wv\",\"wo\",\n",
    "            \"gated_act_proj\",\"mlp_dense_h_to_4h\",\"mlp_dense_4h_to_h\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "    weights = st.load_file(LORA_WEIGHTS, device=\"cpu\")\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# CLEAN OUTPUT\n",
    "# =========================================================\n",
    "def clean_text(text):\n",
    "    text = text.split(\"assistant\")[-1]\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"^#+.*$\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"[-\u2022*]\\s*\", \"\", text)\n",
    "    text = re.sub(\n",
    "        r\"(?i)(image description|pest description|pest identification|prevention methods|analysis).*?:\",\n",
    "        \"\",\n",
    "        text\n",
    "    )\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# =========================================================\n",
    "# ASK FUNCTION\n",
    "# =========================================================\n",
    "def ask(model, img, question):\n",
    "\n",
    "    prompt = (\n",
    "        \"Answer in ONE short paragraph.\\n\"\n",
    "        \"Use plain text only.\\n\"\n",
    "        \"Do not use headings, bullet points, markdown, or labels.\\n\"\n",
    "        \"Stop after completing the answer.\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items() if v is not None}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            repetition_penalty=1.15,\n",
    "            length_penalty=1.1,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    decoded = processor.tokenizer.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return clean_text(decoded)\n",
    "\n",
    "# =========================================================\n",
    "# UPLOAD IMAGE\n",
    "# =========================================================\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# =========================================================\n",
    "# QUESTION\n",
    "# =========================================================\n",
    "question = \"What pest is present in the image and how can it be prevented?\"\n",
    "\n",
    "# =========================================================\n",
    "# RUN ONLY YOUR MODEL\n",
    "# =========================================================\n",
    "model = load_lora_model()\n",
    "\n",
    "print(ask(model, img, question))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228,
     "referenced_widgets": [
      "56ceca27192740f586c9ddcc12ee55f6",
      "b16e64b5f3de416b9ad3213e6bc87570",
      "1519676a6b1e4e06a767aa1210ee3abd",
      "ce30cb46bf3a4b508de79b430ce453c5",
      "123e895d3d27421fbd7a9f595503672d",
      "f8167e7db3534799b913ff6d57943506",
      "a22e4974e62c49c485040d7989b5833c",
      "1a899d5f00d445a18954cb46b92cce6e",
      "c5c0700ee3b548e3bc18cf4783c3c1f3",
      "c583a1587a8142f698ef6b8e51430730",
      "cacdef54d98c40b991f02a6a9c8ed0dd"
     ]
    },
    "id": "XhKnYO89fUqZ",
    "outputId": "bff27e32-192a-4a4e-e9ef-3f5d01490db0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "EVALUATION"
   ],
   "metadata": {
    "id": "qT1qPDE59rWr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------------\n",
    "# LOAD SBERT FOR SEMANTIC SIMILARITY\n",
    "# --------------------------------------------------\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# CLEAN TEXT (GROUND TRUTH + PREDICTION)\n",
    "# --------------------------------------------------\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)   # remove tags\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# EXTRACT GROUND TRUTH FROM DATASET\n",
    "# --------------------------------------------------\n",
    "def extract_ground_truth(conversations):\n",
    "    for turn in conversations:\n",
    "        if turn[\"from\"] == \"assistant\":\n",
    "            return clean_text(turn[\"value\"])\n",
    "    return \"unknown\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# EXTRACT PREDICTED LABEL (KEYWORD MATCH)\n",
    "# --------------------------------------------------\n",
    "LABELS = [\n",
    "    \"aphid\", \"leaf miner\", \"whitefly\", \"thrips\",\n",
    "    \"mite\", \"beetle\", \"caterpillar\",\n",
    "    \"tomato yellow leaf curl virus\", \"powdery mildew\",\n",
    "    \"leaf spot\", \"rust\", \"blight\", \"mosaic virus\"\n",
    "]\n",
    "\n",
    "def extract_label(text):\n",
    "    text = text.lower()\n",
    "    for label in LABELS:\n",
    "        if label in text:\n",
    "            return label\n",
    "    return \"unknown\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# SEMANTIC SIMILARITY\n",
    "# --------------------------------------------------\n",
    "def semantic_similarity(pred, gt):\n",
    "    e1 = sbert.encode(pred, convert_to_tensor=True)\n",
    "    e2 = sbert.encode(gt, convert_to_tensor=True)\n",
    "    return util.cos_sim(e1, e2).item()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# MAIN EVALUATION FUNCTION\n",
    "# --------------------------------------------------\n",
    "def evaluate_model(model, dataset):\n",
    "    y_true, y_pred = [], []\n",
    "    sim_scores = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        image_path = sample[\"image\"]\n",
    "        conversations = sample[\"conversations\"]\n",
    "\n",
    "        # Extract GT\n",
    "        gt_text = extract_ground_truth(conversations)\n",
    "        gt_label = extract_label(gt_text)\n",
    "\n",
    "        # Extract question\n",
    "        user_turn = conversations[0][\"value\"]\n",
    "        question = re.sub(r\"<image>\\s*Question:\\s*\", \"\", user_turn)\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Model inference\n",
    "        pred_text = ask(model, img, question)\n",
    "        pred_text_clean = clean_text(pred_text)\n",
    "        pred_label = extract_label(pred_text_clean)\n",
    "\n",
    "        # Collect\n",
    "        y_true.append(gt_label)\n",
    "        y_pred.append(pred_label)\n",
    "        sim_scores.append(semantic_similarity(pred_text_clean, gt_text))\n",
    "\n",
    "    # Compute F1\n",
    "    labels = sorted(set(y_true + y_pred))\n",
    "    f1 = f1_score(y_true, y_pred, labels=labels, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"f1_score\": f1,\n",
    "        \"semantic_similarity\": float(np.mean(sim_scores))\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------\n",
    "# LOAD DATASET\n",
    "# --------------------------------------------------\n",
    "with open(\"/content/drive/MyDrive/PlantVillage_TestSet/test_dataset50.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# RUN BASE VS LORA\n",
    "# --------------------------------------------------\n",
    "print(\"Evaluating BASE model...\")\n",
    "base_results = evaluate_model(base_model, test_data)\n",
    "\n",
    "print(\"Evaluating LoRA model...\")\n",
    "lora_results = evaluate_model(lora_model, test_data)\n",
    "\n",
    "print(\"\\n===== RESULTS =====\")\n",
    "print(\"BASE:\", base_results)\n",
    "print(\"LORA:\", lora_results)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGnjLPXHGQFE",
    "outputId": "9cd92bde-9ec4-49b2-9c4b-111f99c9958d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "LORA_PATH = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "TEST_JSON = \"/content/drive/MyDrive/PlantVillage_TestSet/test_dataset50.json\"\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD PROCESSOR\n",
    "# ---------------------------------------------------------\n",
    "processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# QUANT CONFIG\n",
    "# ---------------------------------------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD MODELS\n",
    "# ---------------------------------------------------------\n",
    "def load_base():\n",
    "    print(\"Loading BASE model...\")\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_lora():\n",
    "    print(\"Loading LoRA model...\")\n",
    "    base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base, LORA_PATH)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ASK FUNCTION (CLEAN OUTPUT)\n",
    "# ---------------------------------------------------------\n",
    "def ask(model, image, question):\n",
    "    prompt = (\n",
    "        \"Answer based only on what is visible in the image.\\n\"\n",
    "        \"Answer the question.\\n\"\n",
    "        \"Give a short, factual answer.\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items() if v is not None}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    text = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return text.split(\"assistant\")[-1].strip()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FAITHFULNESS SCORER\n",
    "# ---------------------------------------------------------\n",
    "HALLUCINATION_TERMS = [\n",
    "    \"definitely\", \"clearly\", \"confirmed\", \"diagnosis is\",\n",
    "    \"virus\", \"fungal\", \"bacterial\", \"tylcv\", \"blight\"\n",
    "]\n",
    "\n",
    "HEDGING_TERMS = [\n",
    "    \"appears\", \"likely\", \"may\", \"could\", \"suggests\", \"based on visible\"\n",
    "]\n",
    "\n",
    "\n",
    "def faithfulness_score(answer):\n",
    "    answer = answer.lower()\n",
    "\n",
    "    strong_claims = sum(t in answer for t in HALLUCINATION_TERMS)\n",
    "    hedges = sum(t in answer for t in HEDGING_TERMS)\n",
    "\n",
    "    if strong_claims >= 2 and hedges == 0:\n",
    "        return 0\n",
    "    elif strong_claims >= 1 and hedges == 0:\n",
    "        return 1\n",
    "    elif strong_claims >= 1 and hedges >= 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD TEST DATA\n",
    "# ---------------------------------------------------------\n",
    "with open(TEST_JSON) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "QUESTION = \"What is affecting the plant in the image?\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# EVALUATION\n",
    "# ---------------------------------------------------------\n",
    "base_model = load_base()\n",
    "lora_model = load_lora()\n",
    "\n",
    "base_scores = []\n",
    "lora_scores = []\n",
    "\n",
    "for item in tqdm(test_data[:50]):\n",
    "    image_path = item[\"image\"]\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    base_ans = ask(base_model, image, QUESTION)\n",
    "    lora_ans = ask(lora_model, image, QUESTION)\n",
    "\n",
    "    base_scores.append(faithfulness_score(base_ans))\n",
    "    lora_scores.append(faithfulness_score(lora_ans))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RESULTS\n",
    "# ---------------------------------------------------------\n",
    "def summarize(scores):\n",
    "    return {\n",
    "        \"average\": sum(scores) / len(scores),\n",
    "        \"distribution\": {i: scores.count(i) for i in range(4)}\n",
    "    }\n",
    "\n",
    "print(\"\\n===== FAITHFULNESS RESULTS =====\")\n",
    "print(\"BASE:\", summarize(base_scores))\n",
    "print(\"LORA:\", summarize(lora_scores))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "f4076a6bd8f1400da1fdd7ce01e0d6d8",
      "fd027902d50a4345bca4aa59ad2cb1d3",
      "6ea9f200063741eaa96ed8a69d02b82c",
      "cf9bb0f68b09452281114321212be63f",
      "111706a0c947424a88e9047b6f5b79ad",
      "b736e2e97a8745a49ea4e6a081a10591",
      "a5d06c7d2afe425483852cba5d736a66",
      "528de2010c46401a9d366de64d57d173",
      "44cca78e481d4d99abf23c6552e7b40d",
      "1806d2c314ba468b9d694e26975db7d4",
      "62e921019869480e86c2ed42b414648a",
      "410a78010fe44fb28dd3374c26cd8e51",
      "80564f62d0924743b01af4c458de20bc",
      "802fe9bad4b14ce48b7366cf925d5076",
      "a8960721441d43a3828dda01d1df5e65",
      "dd9791135d874dea8b704e363d6b459b",
      "d34c754c74f04c769092e88a4485dd41",
      "029d3e7b83bd4514a23da90dfbeef9cf",
      "3c98a658ca504e86a6d686af3c34d04f",
      "6c69449528d44a28829897a55999f5b0",
      "01328c2d39ea497ca4803c89d6d2f487",
      "b7d777cc2f5544779bff0afec002277f"
     ]
    },
    "id": "ExgcM6PGOvWB",
    "outputId": "06dfb5c6-ecf6-48e9-d76e-b728e399ab8a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import safetensors.torch as st\n",
    "import re\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "LORA_PATH = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "LORA_WEIGHTS = f\"{LORA_PATH}/adapter_model.safetensors\"\n",
    "TEST_JSON = \"/content/drive/MyDrive/PlantVillage_TestSet/test_dataset50.json\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# HALLUCINATION TERMS (NOT DIRECTLY VISIBLE)\n",
    "# ---------------------------------------------------------\n",
    "HALLUCINATION_TERMS = [\n",
    "    \"tree\", \"branch\", \"soil\", \"field\", \"farm\",\n",
    "    \"fungal\", \"bacterial\", \"viral\",\n",
    "    \"tylcv\", \"mosaic\", \"blight\", \"rust\",\n",
    "    \"chlorosis\", \"pathogen\", \"infection\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD PROCESSOR\n",
    "# ---------------------------------------------------------\n",
    "processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD BASE MODEL\n",
    "# ---------------------------------------------------------\n",
    "def load_base():\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD LORA MODEL\n",
    "# ---------------------------------------------------------\n",
    "def load_lora():\n",
    "    base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        peft_type=\"LORA\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "            \"down_proj\",\"up_proj\",\"dense\",\"linear\",\n",
    "            \"proj_in\",\"proj_out\",\"wq\",\"wk\",\"wv\",\"wo\",\n",
    "            \"gated_act_proj\",\"mlp_dense_h_to_4h\",\"mlp_dense_4h_to_h\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    weights = st.load_file(LORA_WEIGHTS, device=\"cpu\")\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ASK FUNCTION (CLEAN, DETERMINISTIC)\n",
    "# ---------------------------------------------------------\n",
    "def ask(model, image, question):\n",
    "    prompt = (\n",
    "        \"Answer only based on what is visible in the image. \"\n",
    "        \"Do not infer causes or name diseases unless directly visible.\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items() if v is not None}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    text = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return text.split(\"assistant\")[-1].strip().lower()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# HALLUCINATION CHECK\n",
    "# ---------------------------------------------------------\n",
    "def is_hallucinated(answer):\n",
    "    return any(term in answer for term in HALLUCINATION_TERMS)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD TEST DATA\n",
    "# ---------------------------------------------------------\n",
    "with open(TEST_JSON, \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# EVALUATION\n",
    "# ---------------------------------------------------------\n",
    "base_model = load_base()\n",
    "lora_model = load_lora()\n",
    "\n",
    "base_hallucinations = 0\n",
    "lora_hallucinations = 0\n",
    "\n",
    "QUESTION = \"What pest or disease is present in the image?\"\n",
    "\n",
    "for sample in tqdm(test_data[:50]):\n",
    "    image_path = sample[\"image\"]\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    base_ans = ask(base_model, image, QUESTION)\n",
    "    lora_ans = ask(lora_model, image, QUESTION)\n",
    "\n",
    "    if is_hallucinated(base_ans):\n",
    "        base_hallucinations += 1\n",
    "    if is_hallucinated(lora_ans):\n",
    "        lora_hallucinations += 1\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RESULTS\n",
    "# ---------------------------------------------------------\n",
    "n = 50\n",
    "print(\"\\n===== VISUAL HALLUCINATION RATE =====\")\n",
    "print(f\"BASE  : {base_hallucinations/n:.3f}\")\n",
    "print(f\"LORA  : {lora_hallucinations/n:.3f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "f59cbb54baa74307bdc446b570209ee7",
      "b91aa4c0b1df48959b64cc03761a508d",
      "1f850ceb363e4594ab8a142fa338aa81",
      "dc576d20d90446a281c511f714b54f0b",
      "dae7fe07e8ed4c3d88ee424b2d3f430d",
      "67cd244bbe164f5ab25648112a198fa3",
      "7604956ac7214d34864ed0c8f832b7d8",
      "e208fc557ace474bb4a0e0731328c020",
      "15545b09d7ac4bd2bc9b79c763f480ee",
      "ec68dc50c0a14f10a800d9dddc0e7788",
      "27d3d4a6fcd84ab7a98edd195edf9a1a",
      "91c573f885b34801901e67bafab86d6d",
      "7f55128a47b94c84881958f7401f5cb0",
      "d1072d6ce96c4e3a8fcd24437ab9232f",
      "efdbc992aa03470e8cfa86e23c6af42f",
      "233f30813deb4eda852349470564b344",
      "5751d18b040646e49a3097e0bff41452",
      "db7e8bfc687341dd935c6746af80f98c",
      "2dbbcec6148445a9969ef1e42aca525f",
      "4020bb50e01e4a0bb479c7ca95277d83",
      "70e26948cf7b41a5abd841bc5a1a4089",
      "0a14d8726b8a4674bb2684c657c67907"
     ]
    },
    "id": "OY4CIP0kRoCA",
    "outputId": "04e52ac5-a9a8-48f9-e365-86c655b0ed47"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch, json, re\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import safetensors.torch as st\n",
    "\n",
    "# =====================================================\n",
    "# PATHS\n",
    "# =====================================================\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "LORA_PATH = \"/content/drive/MyDrive/qwen2vl_grpo_lora_improved_hybrid_fixed\"\n",
    "LORA_WEIGHTS = f\"{LORA_PATH}/adapter_model.safetensors\"\n",
    "TEST_JSON = \"/content/drive/MyDrive/PlantVillage_TestSet/test_dataset50.json\"\n",
    "\n",
    "# =====================================================\n",
    "# PROCESSOR\n",
    "# =====================================================\n",
    "processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# =====================================================\n",
    "# QUANT CONFIG (T4 SAFE)\n",
    "# =====================================================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# LOAD BASE MODEL\n",
    "# =====================================================\n",
    "def load_base():\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =====================================================\n",
    "# LOAD LORA MODEL\n",
    "# =====================================================\n",
    "def load_lora():\n",
    "    base = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        peft_type=\"LORA\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=64,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "            \"down_proj\",\"up_proj\",\"dense\",\"linear\",\n",
    "            \"proj_in\",\"proj_out\",\"wq\",\"wk\",\"wv\",\"wo\",\n",
    "            \"gated_act_proj\",\"mlp_dense_h_to_4h\",\"mlp_dense_4h_to_h\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "    weights = st.load_file(LORA_WEIGHTS, device=\"cpu\")\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =====================================================\n",
    "# QUESTION EXTRACTION\n",
    "# =====================================================\n",
    "def extract_question(conversations):\n",
    "    for turn in conversations:\n",
    "        if turn[\"from\"] == \"user\":\n",
    "            text = turn[\"value\"]\n",
    "            text = text.replace(\"<image>\", \"\").strip()\n",
    "            text = re.sub(r\"Question:\\s*\", \"\", text)\n",
    "            return text.strip()\n",
    "    return \"\"\n",
    "\n",
    "# =====================================================\n",
    "# ANSWER GENERATION\n",
    "# =====================================================\n",
    "def ask(model, img, question):\n",
    "    prompt = (\n",
    "        \"Answer in one short paragraph.\\n\"\n",
    "        \"If the image does not provide enough visual evidence, say so clearly.\\n\\n\"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": img},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items() if v is not None}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    return processor.tokenizer.decode(output[0], skip_special_tokens=True).lower()\n",
    "\n",
    "# =====================================================\n",
    "# METRICS\n",
    "# =====================================================\n",
    "UNSUPPORTED = [\"virus\", \"bacteria\", \"fungal\", \"caused by\", \"tylcv\", \"tmv\"]\n",
    "REFUSALS = [\n",
    "    \"cannot determine\",\n",
    "    \"not enough visual evidence\",\n",
    "    \"cannot be identified\",\n",
    "    \"unclear from the image\"\n",
    "]\n",
    "SPECIFIC = [\"virus\", \"bacteria\", \"fungus\", \"tylcv\", \"tmv\", \"leaf miner\", \"aphid\"]\n",
    "\n",
    "def visual_grounded(ans):\n",
    "    return not any(t in ans for t in UNSUPPORTED)\n",
    "\n",
    "def is_refusal(ans):\n",
    "    return any(t in ans for t in REFUSALS)\n",
    "\n",
    "def osp(ans):\n",
    "    words = ans.split()\n",
    "    return sum(t in ans for t in SPECIFIC) / max(len(words), 1)\n",
    "\n",
    "# =====================================================\n",
    "# AMBIGUITY HEURISTIC\n",
    "# =====================================================\n",
    "def is_ambiguous(question):\n",
    "    q = question.lower()\n",
    "    return any(\n",
    "        kw in q for kw in [\n",
    "            \"which agent\",\n",
    "            \"trace the cause\",\n",
    "            \"diagnosis\",\n",
    "            \"disease\",\n",
    "            \"virus\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# =====================================================\n",
    "# EVALUATION LOOP\n",
    "# =====================================================\n",
    "def evaluate(model, dataset):\n",
    "    vga_score, osp_score = 0, 0\n",
    "    refusal_hits, ambiguous_count = 0, 0\n",
    "\n",
    "    for item in tqdm(dataset):\n",
    "        img = Image.open(item[\"image\"]).convert(\"RGB\")\n",
    "        question = extract_question(item[\"conversations\"])\n",
    "\n",
    "        ans = ask(model, img, question)\n",
    "\n",
    "        vga_score += visual_grounded(ans)\n",
    "        osp_score += osp(ans)\n",
    "\n",
    "        if is_ambiguous(question):\n",
    "            ambiguous_count += 1\n",
    "            refusal_hits += is_refusal(ans)\n",
    "\n",
    "    return {\n",
    "        \"VGA\": vga_score / len(dataset),\n",
    "        \"OSP\": osp_score / len(dataset),\n",
    "        \"RCR\": refusal_hits / max(ambiguous_count, 1)\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# RUN EVALUATION\n",
    "# =====================================================\n",
    "data = json.load(open(TEST_JSON))\n",
    "\n",
    "print(\"\\nLoading BASE model...\")\n",
    "base_model = load_base()\n",
    "\n",
    "print(\"\\nLoading LoRA model...\")\n",
    "lora_model = load_lora()\n",
    "\n",
    "print(\"\\nEvaluating BASE model...\")\n",
    "base_metrics = evaluate(base_model, data)\n",
    "\n",
    "print(\"\\nEvaluating LoRA model...\")\n",
    "lora_metrics = evaluate(lora_model, data)\n",
    "\n",
    "print(\"\\n=========== FINAL RESULTS ===========\")\n",
    "for k in base_metrics:\n",
    "    print(f\"{k:<4} | BASE: {base_metrics[k]:.3f} | LORA: {lora_metrics[k]:.3f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341,
     "referenced_widgets": [
      "7ae485833fab421aa785d3a868379508",
      "106d6466f0624a7aa1c91a8240c2aa66",
      "06a4ca8a2d8a4574ab492fe3a4b74740",
      "ec04cdf7aadd4313aa9eed10f5dc61a1",
      "ba942bad30cd40f3a205d4c34de26923",
      "658237e930f845e88e493173a152b41d",
      "4d677b2564794ebdbe4ae3ce908c0aed",
      "5da64501e07d44ecb236dabf6a0d0028",
      "cd19be2d00e64e3393ff321b8ef9f2c7",
      "9fbb10626e94400a99ed435950c86046",
      "22b5ac06e0c7427b82b8c08810b41508",
      "aecaa612e7294ec5b9c6ccb2ccc2e5f6",
      "885e5edf66cf430bb446147db5c9e298",
      "9c62854aca5a467db03f94b0644ed4a1",
      "86729a003c384eca9a5f95f3ff4128d0",
      "42a0260d9a27477b80f4a7f832a80e1d",
      "3a90d8a9c78440a5b44c6fdf3c3db48b",
      "47e5ed1515e647cb9ad65813622e06b6",
      "32bf64c927464a818ef97390f7969982",
      "fdcb88f0900b4c3db72a1874d6db04c0",
      "9138fe228ec348aebe71558e0ca8e6af",
      "f4a6e29e41a9488ea4ed98dc23efa72c"
     ]
    },
    "id": "SAfNZkyHZyeS",
    "outputId": "78fe9c6d-3e96-4329-afb5-17b0313900b3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Il74qc_OjeC7"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}